{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://gist.github.com/sntaus/b3a90fd7ca74a9b0bdb95a4866693d52"
      ],
      "metadata": {
        "id": "hgOygxls1a7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pprint"
      ],
      "metadata": {
        "id": "I4tPYdsZ0XJd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to obtain training data, vocab and mapping from word to index and vice versa\n",
        "def get_data_and_vocab():\n",
        "    # Define training data\n",
        "    training_data = {\n",
        "        \"how are you\": \"i am fine <end>\",\n",
        "        \"who is john\": \"a nice person <end>\",\n",
        "        \"who is nice\": \"john <end>\",\n",
        "        \"where is john\": \"at home <end>\",\n",
        "        \"how is john\": \"i dont know <end>\",\n",
        "        \"who are you\": \"mini gpt model <end>\"\n",
        "    }\n",
        "\n",
        "    # Extract input and target phrases\n",
        "    data_words = [k for k, _ in training_data.items()]\n",
        "    target_words = [v for _, v in training_data.items()]\n",
        "\n",
        "    # Build vocabulary from training data\n",
        "    vocabulary_words = list(set([element.lower() for nestedlist in [x.split(\" \") for x in data_words] for element in nestedlist] + [element.lower() for nestedlist in [x.split(\" \") for x in target_words] for element in nestedlist]))\n",
        "\n",
        "    # Ensure <end> token is at the end of vocabulary list, and there's a blank at the beginning\n",
        "    vocabulary_words.remove(\"<end>\")\n",
        "    vocabulary_words.append(\"<end>\")\n",
        "    vocabulary_words.insert(0, \"\")\n",
        "\n",
        "    # Create mappings from word to index and index to word\n",
        "    word_to_ix = {vocabulary_words[k].lower(): k for k in range(len(vocabulary_words))}\n",
        "    ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
        "\n",
        "    # Return all the necessary data and mappings\n",
        "    return training_data, data_words, target_words, vocabulary_words, word_to_ix, ix_to_word\n"
      ],
      "metadata": {
        "id": "jET-_Sj70Yh3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a batch of sequences of words to a tensor of indices\n",
        "def words_to_tensor(seq_batch, device=None):\n",
        "    index_batch = []\n",
        "\n",
        "    # Loop over sequences in the batch\n",
        "    for seq in seq_batch:\n",
        "        word_list = seq.lower().split(\" \")\n",
        "        indices = [word_to_ix[word] for word in word_list if word in word_to_ix]\n",
        "        t = torch.tensor(indices)\n",
        "        if device is not None:\n",
        "            t = t.to(device)  # Transfer tensor to the specified device\n",
        "        index_batch.append(t)\n",
        "\n",
        "    # Pad tensors to have the same length\n",
        "    return pad_tensors(index_batch)\n"
      ],
      "metadata": {
        "id": "Bif9f1uk0fXW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a tensor of indices to a list of sequences of words\n",
        "def tensor_to_words(tensor):\n",
        "    index_batch = tensor.cpu().numpy().tolist()\n",
        "    res = []\n",
        "    for indices in index_batch:\n",
        "        words = []\n",
        "        for ix in indices:\n",
        "            words.append(ix_to_word[ix].lower())  # Convert index to word\n",
        "            if ix == word_to_ix[\"<end>\"]:\n",
        "                break  # Stop when <end> token is encountered\n",
        "        res.append(\" \".join(words))\n",
        "    return res"
      ],
      "metadata": {
        "id": "dxHzw8f30hMF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pad a list of tensors to the same length\n",
        "def pad_tensors(list_of_tensors):\n",
        "    tensor_count = len(list_of_tensors) if not torch.is_tensor(list_of_tensors) else list_of_tensors.shape[0]\n",
        "    max_dim = max(t.shape[0] for t in list_of_tensors)  # Find the maximum length\n",
        "    res = []\n",
        "    for t in list_of_tensors:\n",
        "        # Create a zero tensor of the desired shape\n",
        "        res_t = torch.zeros(max_dim, *t.shape[1:]).type(t.dtype).to(t.device)\n",
        "        res_t[:t.shape[0]] = t  # Copy the original tensor into the padded tensor\n",
        "        res.append(res_t)\n",
        "\n",
        "    # Concatenate tensors along a new dimension\n",
        "    res = torch.cat(res)\n",
        "    firstDim = len(list_of_tensors)\n",
        "    secondDim = max_dim\n",
        "\n",
        "    # Reshape the result to have the new dimension first\n",
        "    return res.reshape(firstDim, secondDim, *res.shape[1:])"
      ],
      "metadata": {
        "id": "iQu1YWZi0jtN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define Self-Attention module\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, head_count):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size  # Size of word embeddings\n",
        "        self.head_count = head_count  # Number of attention heads\n",
        "\n",
        "        # Create linear layers for query, key and value projections for each head\n",
        "        self.query_layers = nn.ModuleList([nn.Linear(embed_size, embed_size, bias=False) for _ in range(head_count)])\n",
        "        self.key_layers = nn.ModuleList([nn.Linear(embed_size, embed_size, bias=False) for _ in range(head_count)])\n",
        "        self.value_layers = nn.ModuleList([nn.Linear(embed_size, embed_size, bias=False) for _ in range(head_count)])\n",
        "        self.fc_out = nn.Linear(head_count * embed_size, embed_size)  # Final linear layer to combine head outputs\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        batch_size, token_count = embeddings.shape[:2]\n",
        "        qkvs = torch.zeros(self.head_count, 3, batch_size, token_count, self.embed_size).to(embeddings.device)\n",
        "\n",
        "        # Loop over heads and compute query, key and value projections\n",
        "        for i in range(self.head_count):\n",
        "            qkvs[i, 0] = self.query_layers[i](embeddings)\n",
        "            qkvs[i, 1] = self.key_layers[i](embeddings)\n",
        "            qkvs[i, 2] = self.value_layers[i](embeddings)\n",
        "\n",
        "        # Compute energy terms for each head, batch, and pair of tokens\n",
        "        energy = torch.zeros(self.head_count, batch_size, token_count, token_count).to(embeddings.device)\n",
        "        # Create a mask with false on and below the diagonal, and true above the diagonal\n",
        "        mask = torch.triu(torch.ones((token_count, token_count)), diagonal=1).bool()\n",
        "\n",
        "        for h in range(self.head_count):\n",
        "            for b in range(batch_size):\n",
        "                for i in range(token_count):\n",
        "                    for j in range(token_count):\n",
        "                        energy[h, b, i, j] = torch.dot(qkvs[h, 0, b, i], qkvs[h, 1, b, j])\n",
        "                energy[h, b] = energy[h, b].masked_fill(mask, float('-inf')) # Apply mask\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention = torch.nn.functional.softmax(energy, dim=3)\n",
        "\n",
        "        # Compute weighted sum of values for each head and token\n",
        "        out = torch.zeros(batch_size, token_count, self.head_count, self.embed_size).to(embeddings.device)\n",
        "        for h in range(self.head_count):\n",
        "            for b in range(batch_size):\n",
        "                for i in range(token_count):\n",
        "                    for j in range(token_count):\n",
        "                        out[b, i, h] += (attention[h, b, i, j] * qkvs[h, 2, b, j])\n",
        "\n",
        "        # Reshape and pass through final linear layer\n",
        "        out = out.reshape(batch_size, token_count, self.head_count * self.embed_size)\n",
        "        return self.fc_out(out)\n",
        "\n",
        "    def masked_attention(self, energy):\n",
        "        # Assume scores has shape (batch_size, max_token_count, embed_size, embed_size)\n",
        "        max_token_count, embed_size, _ = energy.size()\n",
        "\n",
        "        # Create a mask with zeros on and below the diagonal, and negative infinity above the diagonal\n",
        "        mask = torch.triu(torch.ones((max_token_count, max_token_count)), diagonal=1) * float('-inf')\n",
        "        mask = mask.unsqueeze(0).unsqueeze(0)  # Add dimensions for batch and embedding size\n",
        "        mask = mask.expand(batch_size, embed_size, -1, -1)  # Expand mask to match batch and embedding size\n",
        "\n",
        "        # Apply the mask to the scores\n",
        "        masked_scores = energy + mask.to(energy.device)\n",
        "\n",
        "        return masked_scores.to(energy.device)"
      ],
      "metadata": {
        "id": "YucvWsW80sgn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define Transformer block module\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, head_count):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embed_size, head_count)  # Self-attention layer\n",
        "        self.norm1 = nn.LayerNorm(embed_size)  # Layer normalization\n",
        "        self.norm2 = nn.LayerNorm(embed_size)  # Layer normalization\n",
        "\n",
        "        # Feed-forward neural network\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_size, embed_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        attention = self.attention(embeddings)\n",
        "\n",
        "        # Apply residual connections and layer normalization\n",
        "        out = self.norm1(attention + embeddings)\n",
        "        out = attention + self.feed_forward(out)\n",
        "        out = self.norm2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "PMjdfZ1l0uZG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define Transformer module\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_layers, head_count):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed_size = embed_size  # Size of word embeddings\n",
        "        self.vocab_size = vocab_size  # Size of vocabulary\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size)  # Embedding layer\n",
        "\n",
        "        # List of transformer blocks\n",
        "        self.layers = nn.ModuleList(\n",
        "            [TransformerBlock(embed_size, head_count) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)  # Final linear layer to produce logits\n",
        "\n",
        "    def forward(self, input_tokens, mask=None):\n",
        "        batch_size, token_count = input_tokens.shape[:2]\n",
        "        out = self.word_embedding(input_tokens)  # Obtain word embeddings\n",
        "\n",
        "        # Compute position encodings and add to word embeddings\n",
        "        positions = torch.arange(0, token_count).expand(batch_size, token_count).to(input_tokens.device)\n",
        "        position_encoding = self.position_encoding(positions, self.embed_size)\n",
        "        out += position_encoding.reshape(out.shape)\n",
        "\n",
        "        # Pass through each transformer block\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "\n",
        "        # Produce logits for the final token in each sequence\n",
        "        out = self.fc_out(out[:, -1, :].reshape(batch_size, self.embed_size)).reshape(batch_size, self.vocab_size)\n",
        "        return torch.nn.functional.softmax(out, dim=1)  # Apply softmax to obtain probabilities\n",
        "\n",
        "    def position_encoding(self, positions, embed_size):\n",
        "        # Compute position encoding for each position and dimension\n",
        "        angle_rads = self.get_angles(\n",
        "            positions.unsqueeze(2).float(),\n",
        "            torch.arange(embed_size)[None, None, :].float().to(positions.device),\n",
        "            embed_size\n",
        "        )\n",
        "        sines = torch.sin(angle_rads[:, :, 0::2])  # Compute sine of angle for even dimensions\n",
        "        cosines = torch.cos(angle_rads[:, :, 1::2])  # Compute cosine of angle for odd dimensions\n",
        "        pos_encoding = torch.cat([sines, cosines], dim=-1)  # Concatenate sine and cosine values\n",
        "        pos_encoding = pos_encoding[None, ...]\n",
        "        return pos_encoding\n",
        "\n",
        "    def get_angles(self, pos, i, embed_size):\n",
        "        # Compute angle rate for each position and dimension\n",
        "        angle_rates = 1 / torch.pow(10000, (2 * (i//2)) / embed_size)\n",
        "        return pos * angle_rates"
      ],
      "metadata": {
        "id": "6qnWaFFH0w2W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model recursively over each sequence and token\n",
        "def train_recursive(model, data, targets, optimizer, criterion):\n",
        "    model.train()  # Set model to training mode\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    total_loss = 0  # Initialize total loss\n",
        "    batch_size, token_count, token_count_out = data.shape[0], data.shape[1], targets.shape[1]\n",
        "\n",
        "    # Loop over sequences in the batch\n",
        "    for b in range(batch_size):\n",
        "        end_encountered = False\n",
        "        cur_count = 0\n",
        "        # Loop over tokens in the sequence\n",
        "        while not end_encountered:\n",
        "            target_vector = torch.zeros(model.vocab_size).to(data.device)  # Initialize target vector\n",
        "\n",
        "            if cur_count != token_count_out:\n",
        "                expected_next_token_idx = targets[b, cur_count]  # Get index of expected next token\n",
        "                target_vector[expected_next_token_idx] = 1  # Set the corresponding element of the target vector to 1\n",
        "\n",
        "            # Concatenate current input and output tokens and pass through model\n",
        "            if cur_count > 0:\n",
        "                model_input = data[b].reshape(token_count).to(data.device)\n",
        "                part_of_output = targets[b, :cur_count].to(data.device)\n",
        "                model_input = torch.cat((model_input, part_of_output))\n",
        "            else:\n",
        "                model_input = data[b]\n",
        "            out = model(model_input.reshape(1, token_count + cur_count))\n",
        "\n",
        "            # Compute loss and accumulate total loss\n",
        "            loss = criterion(out, target_vector.reshape(out.shape))\n",
        "            total_loss += loss\n",
        "            cur_count += 1\n",
        "\n",
        "            # Stop when the end of the sequence is reached\n",
        "            if cur_count > token_count_out:\n",
        "                end_encountered = True\n",
        "\n",
        "    # Backpropagate gradients and update model parameters\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    return total_loss.item() / batch_size"
      ],
      "metadata": {
        "id": "fujLJs_R0ze1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform inference recursively for each sequence in a batch\n",
        "def infer_recursive(model, input_vectors, max_output_token_count=10):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    outputs = []\n",
        "\n",
        "    # Loop over sequences in the batch\n",
        "    for i in range(input_vectors.shape[0]):\n",
        "        print(f\"Infering sequence {i}\")\n",
        "        input_vector = input_vectors[i].reshape(1, input_vectors.shape[1])\n",
        "        predicted_sequence = []\n",
        "        wc = 0  # Initialize word count\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation\n",
        "            while True:\n",
        "                output = model(input_vector)  # Pass current input through model\n",
        "                predicted_index = output[0, :].argmax().item()  # Get index of predicted token\n",
        "                predicted_sequence.append(predicted_index)  # Append predicted index to sequence\n",
        "                # Stop when <end> token is predicted or the maximum output length is reached\n",
        "                if predicted_index == word_to_ix['<end>'] or wc > max_output_token_count:\n",
        "                    break\n",
        "                # Append predicted token to input and increment word count\n",
        "                input_vector = torch.cat([input_vector, torch.tensor([[predicted_index]])], dim=1)\n",
        "                wc += 1\n",
        "        outputs.append(torch.tensor(predicted_sequence))  # Append predicted sequence to outputs\n",
        "    outputs = pad_tensors(outputs)  # Pad predicted sequences to the same length\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "v0y0fADy01au"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ3o-i8_0J8y",
        "outputId": "c1a51cf9-d30e-4f62-837c-de490d8dfc98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 12.3157\n",
            "Epoch 2, Loss: 12.2033\n",
            "Epoch 3, Loss: 12.0552\n",
            "Epoch 4, Loss: 11.8978\n",
            "Epoch 5, Loss: 11.7320\n",
            "Epoch 6, Loss: 11.5944\n",
            "Epoch 7, Loss: 11.4813\n",
            "Epoch 8, Loss: 11.3909\n",
            "Epoch 9, Loss: 11.3140\n",
            "Epoch 10, Loss: 11.2376\n",
            "Epoch 11, Loss: 11.1702\n",
            "Epoch 12, Loss: 11.1141\n",
            "Epoch 13, Loss: 11.0624\n",
            "Epoch 14, Loss: 11.0141\n",
            "Epoch 15, Loss: 10.9705\n",
            "Epoch 16, Loss: 10.9308\n",
            "Epoch 17, Loss: 10.8915\n",
            "Epoch 18, Loss: 10.8599\n",
            "Epoch 19, Loss: 10.8306\n",
            "Epoch 20, Loss: 10.8001\n",
            "Epoch 21, Loss: 10.7657\n",
            "Epoch 22, Loss: 10.7257\n",
            "Epoch 23, Loss: 10.6800\n",
            "Epoch 24, Loss: 10.6286\n",
            "Epoch 25, Loss: 10.5735\n",
            "Epoch 26, Loss: 10.5205\n",
            "Epoch 27, Loss: 10.4736\n",
            "Epoch 28, Loss: 10.4336\n",
            "Epoch 29, Loss: 10.3977\n",
            "Epoch 30, Loss: 10.3619\n",
            "Epoch 31, Loss: 10.3266\n",
            "Epoch 32, Loss: 10.2932\n",
            "Epoch 33, Loss: 10.2685\n",
            "Epoch 34, Loss: 10.2498\n",
            "Epoch 35, Loss: 10.2354\n",
            "Epoch 36, Loss: 10.2241\n",
            "Epoch 37, Loss: 10.2149\n",
            "Epoch 38, Loss: 10.2073\n",
            "Epoch 39, Loss: 10.2005\n",
            "Epoch 40, Loss: 10.1944\n",
            "Epoch 41, Loss: 10.1888\n",
            "Epoch 42, Loss: 10.1834\n",
            "Epoch 43, Loss: 10.1785\n",
            "Epoch 44, Loss: 10.1739\n",
            "Epoch 45, Loss: 10.1694\n",
            "Epoch 46, Loss: 10.1648\n",
            "Epoch 47, Loss: 10.1596\n",
            "Epoch 48, Loss: 10.1531\n",
            "Epoch 49, Loss: 10.1438\n",
            "Epoch 50, Loss: 10.1294\n",
            "Epoch 51, Loss: 10.1056\n",
            "Epoch 52, Loss: 10.0714\n",
            "Epoch 53, Loss: 10.0364\n",
            "Epoch 54, Loss: 10.0122\n",
            "Epoch 55, Loss: 9.9988\n",
            "Infering sequence 0\n",
            "Infering sequence 1\n",
            "Infering sequence 2\n",
            "Infering sequence 3\n",
            "Infering sequence 4\n",
            "Infering sequence 5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Training Data:\n",
            "{'how are you': 'i am fine <end>',\n",
            " 'how is john': 'i dont know <end>',\n",
            " 'where is john': 'at home <end>',\n",
            " 'who are you': 'mini gpt model <end>',\n",
            " 'who is john': 'a nice person <end>',\n",
            " 'who is nice': 'john <end>'}\n",
            "\n",
            "\n",
            "\n",
            "Model Inference:\n",
            "{'how are you': 'am am am am am am am dont am dont am dont',\n",
            " 'how is john': 'dont dont dont dont dont dont dont dont dont dont dont dont',\n",
            " 'where is john': 'at home <end>',\n",
            " 'who are you': 'mini gpt model <end>',\n",
            " 'who is john': 'person <end>',\n",
            " 'who is nice': 'person <end>'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Function to demonstrate training and inference\n",
        "def example_training_and_inference():\n",
        "    # Get model hyperparameters from vocabulary size\n",
        "    vocab_size = len(word_to_ix)\n",
        "    embed_size = 512\n",
        "    num_layers = 4\n",
        "    heads = 3\n",
        "\n",
        "    # Create model, optimizer, and loss function\n",
        "    device = torch.device(\"cpu\")\n",
        "    model = Transformer(vocab_size, embed_size, num_layers, heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Convert training data to tensors\n",
        "    data = words_to_tensor(data_words, device=device)\n",
        "    targets = words_to_tensor(target_words, device=device)\n",
        "\n",
        "    # Train model for 55 epochs\n",
        "    for epoch in range(55):\n",
        "        avg_loss = train_recursive(model, data, targets, optimizer, criterion)\n",
        "        print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Perform inference on training data\n",
        "    input_vector = words_to_tensor(data_words, device=device)\n",
        "    predicted_vector = infer_recursive(model, input_vector)\n",
        "    predicted_words = tensor_to_words(predicted_vector)\n",
        "\n",
        "    # Print training data and model output\n",
        "    print(\"\\n\\n\\n\")\n",
        "    print(\"Training Data:\")\n",
        "    pprint.pprint(training_data)\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"Model Inference:\")\n",
        "    result_data = {data_words[k]: predicted_words[k] for k in range(len(predicted_words))}\n",
        "    pprint.pprint(result_data)\n",
        "\n",
        "# Main function to call the demonstration function\n",
        "if __name__ == \"__main__\":\n",
        "    # Get training data and vocabulary\n",
        "    training_data, data_words, target_words, vocabulary_words, word_to_ix, ix_to_word = get_data_and_vocab()\n",
        "    # Run the example training and inference function\n",
        "    example_training_and_inference()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to demonstrate training and inference\n",
        "def example_training_and_inference():\n",
        "    # Get model hyperparameters from vocabulary size\n",
        "    vocab_size = len(word_to_ix)\n",
        "    embed_size = 512\n",
        "    num_layers = 4\n",
        "    heads = 3\n",
        "\n",
        "    # Create model, optimizer, and loss function\n",
        "    device = torch.device(\"cpu\")\n",
        "    model = Transformer(vocab_size, embed_size, num_layers, heads).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Convert training data to tensors\n",
        "    data = words_to_tensor(data_words, device=device)\n",
        "    targets = words_to_tensor(target_words, device=device)\n",
        "\n",
        "    # Train model for 55 epochs\n",
        "    for epoch in range(500):\n",
        "        avg_loss = train_recursive(model, data, targets, optimizer, criterion)\n",
        "        print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Perform inference on training data\n",
        "    input_vector = words_to_tensor(data_words, device=device)\n",
        "    predicted_vector = infer_recursive(model, input_vector)\n",
        "    predicted_words = tensor_to_words(predicted_vector)\n",
        "\n",
        "    # Print training data and model output\n",
        "    print(\"\\n\\n\\n\")\n",
        "    print(\"Training Data:\")\n",
        "    pprint.pprint(training_data)\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"Model Inference:\")\n",
        "    result_data = {data_words[k]: predicted_words[k] for k in range(len(predicted_words))}\n",
        "    pprint.pprint(result_data)\n",
        "\n",
        "# Main function to call the demonstration function\n",
        "if __name__ == \"__main__\":\n",
        "    # Get training data and vocabulary\n",
        "    training_data, data_words, target_words, vocabulary_words, word_to_ix, ix_to_word = get_data_and_vocab()\n",
        "    # Run the example training and inference function\n",
        "    example_training_and_inference()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTIVjS3q1HFZ",
        "outputId": "5cf709c1-d8c6-4c7d-ab6f-1dc9a49fb426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 12.4027\n",
            "Epoch 2, Loss: 12.3289\n",
            "Epoch 3, Loss: 12.2368\n",
            "Epoch 4, Loss: 12.1112\n",
            "Epoch 5, Loss: 11.9609\n",
            "Epoch 6, Loss: 11.7946\n",
            "Epoch 7, Loss: 11.6304\n",
            "Epoch 8, Loss: 11.4818\n",
            "Epoch 9, Loss: 11.3652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JKeTMTh122Ur"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}